# Which LLM server to talk to (local, lab, DRAC tunnel, etc.)
LLM_BASE_URL=http://127.0.0.1:1234

# If your server requires a key; if not, can be anything
LLM_API_KEY=local-dev

# Whatever name the server exposes for the model
LLM_MODEL=meta-llama-3.1-8b-instruct

# Generation settings
LLM_TEMPERATURE=0
LLM_MAX_TOKENS=1500

# Optional: request timeout (seconds)
LLM_TIMEOUT=120